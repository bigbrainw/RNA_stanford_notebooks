{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":118765,"databundleVersionId":15231210,"sourceType":"competition"},{"sourceId":11118830,"sourceType":"datasetVersion","datasetId":6933267},{"sourceId":292115284,"sourceType":"kernelVersion"},{"sourceId":311741,"sourceType":"modelInstanceVersion","modelInstanceId":264400,"modelId":285488}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3061.935344,"end_time":"2026-01-15T23:04:47.93547","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-15T22:13:46.000126","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§¬RNAPro: An accurate RNA structure prediction model by Kaggle synthesis\n\n**About:**\nThis notebooks shows how to use RNAPro for inference\n\nâ­ The code is available on GitHub â­\n> https://github.com/NVIDIA-Digital-Bio/RNAPro","metadata":{"papermill":{"duration":0.006278,"end_time":"2026-01-15T22:13:48.680723","exception":false,"start_time":"2026-01-15T22:13:48.674445","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Download everything\n\n> This requires internet\n\nFor offline use, your notebook can use the output of the following cells as an external source, and proceed to the installation section","metadata":{"papermill":{"duration":0.004739,"end_time":"2026-01-15T22:13:48.690693","exception":false,"start_time":"2026-01-15T22:13:48.685954","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Weights","metadata":{"papermill":{"duration":0.004687,"end_time":"2026-01-15T22:13:48.700197","exception":false,"start_time":"2026-01-15T22:13:48.69551","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!wget https://huggingface.co/nvidia/RNAPro-Private-Best-500M/resolve/main/rnapro-private-best-500m.ckpt","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2026-01-15T23:11:26.676176Z","iopub.execute_input":"2026-01-15T23:11:26.676403Z"},"papermill":{"duration":1975.401231,"end_time":"2026-01-15T22:46:44.106287","exception":false,"start_time":"2026-01-15T22:13:48.705056","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Code","metadata":{"papermill":{"duration":0.167493,"end_time":"2026-01-15T22:46:44.436316","exception":false,"start_time":"2026-01-15T22:46:44.268823","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA-Digital-Bio/RNAPro.git","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:46:44.767559Z","iopub.status.busy":"2026-01-15T22:46:44.767276Z","iopub.status.idle":"2026-01-15T22:46:45.670352Z","shell.execute_reply":"2026-01-15T22:46:45.669562Z"},"papermill":{"duration":1.070913,"end_time":"2026-01-15T22:46:45.672274","exception":false,"start_time":"2026-01-15T22:46:44.601361","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Package Wheels","metadata":{"papermill":{"duration":0.161265,"end_time":"2026-01-15T22:46:46.000871","exception":false,"start_time":"2026-01-15T22:46:45.839606","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!mkdir wheels","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:46:46.330241Z","iopub.status.busy":"2026-01-15T22:46:46.329481Z","iopub.status.idle":"2026-01-15T22:46:46.447555Z","shell.execute_reply":"2026-01-15T22:46:46.44661Z"},"papermill":{"duration":0.284767,"end_time":"2026-01-15T22:46:46.44955","exception":false,"start_time":"2026-01-15T22:46:46.164783","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd wheels","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:46:46.87367Z","iopub.status.busy":"2026-01-15T22:46:46.872922Z","iopub.status.idle":"2026-01-15T22:46:46.879185Z","shell.execute_reply":"2026-01-15T22:46:46.878569Z"},"papermill":{"duration":0.178695,"end_time":"2026-01-15T22:46:46.880764","exception":false,"start_time":"2026-01-15T22:46:46.702069","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip download -r ../RNAPro/requirements.txt","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2026-01-15T22:46:47.223408Z","iopub.status.busy":"2026-01-15T22:46:47.223096Z","iopub.status.idle":"2026-01-15T22:48:47.552833Z","shell.execute_reply":"2026-01-15T22:48:47.551849Z"},"papermill":{"duration":120.501337,"end_time":"2026-01-15T22:48:47.554536","exception":false,"start_time":"2026-01-15T22:46:47.053199","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Installation\n\nIf you are using an offline version, copy the RNAPro folder to `/kaggle/working/` first.\nYou will also need to update the `--find-links=./wheels/` argument.","metadata":{"papermill":{"duration":0.216564,"end_time":"2026-01-15T22:48:47.98686","exception":false,"start_time":"2026-01-15T22:48:47.770296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cd /kaggle/working/","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:48:48.531543Z","iopub.status.busy":"2026-01-15T22:48:48.530658Z","iopub.status.idle":"2026-01-15T22:48:48.536219Z","shell.execute_reply":"2026-01-15T22:48:48.535517Z"},"papermill":{"duration":0.239616,"end_time":"2026-01-15T22:48:48.537648","exception":false,"start_time":"2026-01-15T22:48:48.298032","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -r RNAPro/requirements.txt --find-links=./wheels/","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2026-01-15T22:48:48.983198Z","iopub.status.busy":"2026-01-15T22:48:48.982699Z","iopub.status.idle":"2026-01-15T22:52:14.803496Z","shell.execute_reply":"2026-01-15T22:52:14.802571Z"},"papermill":{"duration":206.03569,"end_time":"2026-01-15T22:52:14.805078","exception":false,"start_time":"2026-01-15T22:48:48.769388","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd RNAPro","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:15.322197Z","iopub.status.busy":"2026-01-15T22:52:15.321392Z","iopub.status.idle":"2026-01-15T22:52:15.326699Z","shell.execute_reply":"2026-01-15T22:52:15.325963Z"},"papermill":{"duration":0.225636,"end_time":"2026-01-15T22:52:15.328151","exception":false,"start_time":"2026-01-15T22:52:15.102515","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -e . --no-deps","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:15.767642Z","iopub.status.busy":"2026-01-15T22:52:15.767358Z","iopub.status.idle":"2026-01-15T22:52:20.593876Z","shell.execute_reply":"2026-01-15T22:52:20.592861Z"},"papermill":{"duration":5.048747,"end_time":"2026-01-15T22:52:20.595672","exception":false,"start_time":"2026-01-15T22:52:15.546925","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Templates\n\nFor simplicity, I use precomputed templates. When submitting, you will need to compute templates as part of your inference notebook.\n\nThe code to compute templates is available here :\n> https://www.kaggle.com/code/theoviel/stanford-rna-3d-folding-part-2-templates/\n\nWe simply call the utility script to convert them to the expected format.","metadata":{"papermill":{"duration":0.217386,"end_time":"2026-01-15T22:52:21.051653","exception":false,"start_time":"2026-01-15T22:52:20.834267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!python preprocess/convert_templates_to_pt_files.py --input_csv /kaggle/input/stanford-rna-3d-folding-part-2-templates/templates_tbm.csv --output_name templates.pt","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:21.492845Z","iopub.status.busy":"2026-01-15T22:52:21.492496Z","iopub.status.idle":"2026-01-15T22:52:25.155735Z","shell.execute_reply":"2026-01-15T22:52:25.155041Z"},"papermill":{"duration":3.887097,"end_time":"2026-01-15T22:52:25.157531","exception":false,"start_time":"2026-01-15T22:52:21.270434","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## cdd cache\n\nYou can either \n- Recompute the cdd cache using `python preprocess/gen_ccd_cache.py`\n- Use the precomputed files from the proteinix external dataset.\n\nRunning the script might be better since the resulting file will include the latest information, but requires some time.","metadata":{"papermill":{"duration":0.214123,"end_time":"2026-01-15T22:52:25.673436","exception":false,"start_time":"2026-01-15T22:52:25.459313","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DIST = \"/kaggle/working/RNAPro/release_data/ccd_cache/\"\n!mkdir -p $DIST","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:26.107847Z","iopub.status.busy":"2026-01-15T22:52:26.107268Z","iopub.status.idle":"2026-01-15T22:52:26.224871Z","shell.execute_reply":"2026-01-15T22:52:26.224109Z"},"papermill":{"duration":0.336228,"end_time":"2026-01-15T22:52:26.226391","exception":false,"start_time":"2026-01-15T22:52:25.890163","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python preprocess/gen_ccd_cache.py -c $DIST","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:26.660188Z","iopub.status.busy":"2026-01-15T22:52:26.659387Z","iopub.status.idle":"2026-01-15T22:52:26.663436Z","shell.execute_reply":"2026-01-15T22:52:26.662678Z"},"papermill":{"duration":0.223126,"end_time":"2026-01-15T22:52:26.665028","exception":false,"start_time":"2026-01-15T22:52:26.441902","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/input/protenix-checkpoints/components.v20240608.cif $DIST\n!cp /kaggle/input/protenix-checkpoints/components.v20240608.cif.rdkit_mol.pkl $DIST","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:27.103897Z","iopub.status.busy":"2026-01-15T22:52:27.103248Z","iopub.status.idle":"2026-01-15T22:52:32.721603Z","shell.execute_reply":"2026-01-15T22:52:32.720544Z"},"papermill":{"duration":5.839296,"end_time":"2026-01-15T22:52:32.72341","exception":false,"start_time":"2026-01-15T22:52:26.884114","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference\n","metadata":{"papermill":{"duration":0.308629,"end_time":"2026-01-15T22:52:33.251757","exception":false,"start_time":"2026-01-15T22:52:32.943128","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cd /kaggle/working/RNAPro","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:33.689456Z","iopub.status.busy":"2026-01-15T22:52:33.688864Z","iopub.status.idle":"2026-01-15T22:52:33.693502Z","shell.execute_reply":"2026-01-15T22:52:33.692853Z"},"papermill":{"duration":0.224458,"end_time":"2026-01-15T22:52:33.695085","exception":false,"start_time":"2026-01-15T22:52:33.470627","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Subsample of the data for inference example","metadata":{"papermill":{"duration":0.216091,"end_time":"2026-01-15T22:52:34.127422","exception":false,"start_time":"2026-01-15T22:52:33.911331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%python\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv\")\ndf = df.head(5)\ndf.to_csv('/kaggle/working/sample_sequences.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:34.566142Z","iopub.status.busy":"2026-01-15T22:52:34.565839Z","iopub.status.idle":"2026-01-15T22:52:35.169438Z","shell.execute_reply":"2026-01-15T22:52:35.168846Z"},"papermill":{"duration":0.824995,"end_time":"2026-01-15T22:52:35.171136","exception":false,"start_time":"2026-01-15T22:52:34.346141","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Updated runner\n- Modified from the original repo for smoother use on Kaggle","metadata":{"papermill":{"duration":0.303613,"end_time":"2026-01-15T22:52:35.692572","exception":false,"start_time":"2026-01-15T22:52:35.388959","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile runner/inference.py\n# SPDX-FileCopyrightText: Copyright (c) 2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport logging\nimport traceback\nimport warnings\nimport argparse\nfrom contextlib import nullcontext\nfrom os.path import join as opjoin\nfrom typing import Any, Mapping\n\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom biotite.structure.io import pdbx\n\nfrom configs.configs_base import configs as configs_base\nfrom configs.configs_data import data_configs\nfrom configs.configs_inference import inference_configs\nfrom runner.dumper import DataDumper\n\nfrom rnapro.config import parse_sys_args\nfrom rnapro.config.config import ConfigManager, ArgumentNotSet\nfrom rnapro.data.infer_data_pipeline import get_inference_dataloader\nfrom rnapro.model.RNAPro import RNAPro\nfrom rnapro.utils.distributed import DIST_WRAPPER\nfrom rnapro.utils.seed import seed_everything\nfrom rnapro.utils.torch_utils import to_device\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nlogger = logging.getLogger(__name__)\n\n# Silence all info logging\nlogging.basicConfig(level=logging.WARNING)\n# Silence dataloader logging specifically\nlogging.getLogger(\"rnapro.data\").setLevel(logging.WARNING)\nlogging.getLogger(\"rnapro\").setLevel(logging.WARNING)\n\n\ndef parse_configs(\n    configs: dict, arg_str: str = None, fill_required_with_null: bool = False\n):\n    \"\"\"\n    Parses and merges configuration settings from a dictionary and command-line arguments.\n\n    Args:\n        configs (dict): A dictionary containing initial configuration settings.\n        arg_str (str, optional): A string representing command-line arguments. Defaults to None.\n        fill_required_with_null (bool, optional):\n            A boolean flag indicating whether required values should be filled with `None` if not provided. Defaults to False.\n\n    Returns:\n        ConfigDict: The merged configuration dictionary.\n    \"\"\"\n    manager = ConfigManager(configs, fill_required_with_null=fill_required_with_null)\n    parser = argparse.ArgumentParser()\n\n    # This is new\n    parser.add_argument(\n        \"--max_len\",\n        type=int,\n        default=10000,\n        required=False,\n        help=\"Maximum length of the sequence. Longer sequences will be skipped during inference\"\n    )\n\n    # Register arguments\n    for key, (\n        dtype,\n        default_value,\n        allow_none,\n        required,\n    ) in manager.config_infos.items():\n        # All config use str type, strings will be converted to real dtype later\n        parser.add_argument(\n            \"--\" + key, type=str, default=ArgumentNotSet(), required=required\n        )\n    # Merge user commandline pargs with default ones\n    merged_configs = manager.merge_configs(\n        vars(parser.parse_args(arg_str.split())) if arg_str else {}\n    )\n\n    max_len = parser.parse_args(arg_str.split()).max_len\n    merged_configs.max_len = max_len\n\n    return merged_configs\n\n\nclass dotdict(dict):\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __getattr__(self, name):\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(name)\n\n\nclass InferenceRunner(object):\n    def __init__(self, configs: Any) -> None:\n        self.configs = configs\n        self.init_env()\n        self.init_basics()\n        self.init_model()\n        self.load_checkpoint()\n        self.init_dumper(\n            need_atom_confidence=configs.need_atom_confidence,\n            sorted_by_ranking_score=configs.sorted_by_ranking_score,\n        )\n\n    def init_env(self) -> None:\n        self.print(\n            f\"Distributed environment: world size: {DIST_WRAPPER.world_size}, \"\n            + f\"global rank: {DIST_WRAPPER.rank}, local rank: {DIST_WRAPPER.local_rank}\"\n        )\n        self.use_cuda = torch.cuda.device_count() > 0\n        if self.use_cuda:\n            self.device = torch.device(\"cuda:{}\".format(DIST_WRAPPER.local_rank))\n            os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n            all_gpu_ids = \",\".join(str(x) for x in range(torch.cuda.device_count()))\n            devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", all_gpu_ids)\n            logging.info(\n                f\"LOCAL_RANK: {DIST_WRAPPER.local_rank} - CUDA_VISIBLE_DEVICES: [{devices}]\"\n            )\n            torch.cuda.set_device(self.device)\n        else:\n            self.device = torch.device(\"cpu\")\n        if self.configs.use_deepspeed_evo_attention:\n            env = os.getenv(\"CUTLASS_PATH\", None)\n            self.print(f\"env: {env}\")\n            assert (\n                env is not None\n            ), \"if use ds4sci, set `CUTLASS_PATH` env as https://www.deepspeed.ai/tutorials/ds4sci_evoformerattention/\"\n            if env is not None:\n                logging.info(\n                    \"The kernels will be compiled when DS4Sci_EvoformerAttention is called for the first time.\"\n                )\n        use_fastlayernorm = os.getenv(\"LAYERNORM_TYPE\", None)\n        if use_fastlayernorm == \"fast_layernorm\":\n            logging.info(\n                \"The kernels will be compiled when fast_layernorm is called for the first time.\"\n            )\n\n        logging.info(\"Finished init ENV.\")\n\n    def init_basics(self) -> None:\n        self.dump_dir = self.configs.dump_dir\n        self.error_dir = opjoin(self.dump_dir, \"ERR\")\n        os.makedirs(self.dump_dir, exist_ok=True)\n        os.makedirs(self.error_dir, exist_ok=True)\n\n    def init_model(self) -> None:\n        self.model = RNAPro(self.configs).to(self.device)\n        num_params = sum(p.numel() for p in self.model.parameters())\n        self.print(f\"Total number of parameters: {num_params:,}\")\n\n    def load_checkpoint(self) -> None:\n        checkpoint_path = self.configs.load_checkpoint_path\n\n        if not os.path.exists(checkpoint_path):\n            raise Exception(f\"Given checkpoint path not exist [{checkpoint_path}]\")\n        self.print(\n            f\"Loading from {checkpoint_path}, strict: {self.configs.load_strict}\"\n        )\n        checkpoint = torch.load(checkpoint_path, self.device)\n\n        sample_key = [k for k in checkpoint[\"model\"].keys()][0]\n        # self.print(f\"Sampled key: {sample_key}\")\n        if sample_key.startswith(\"module.\"):  # DDP checkpoint has module. prefix\n            checkpoint[\"model\"] = {\n                k[len(\"module.\"):]: v for k, v in checkpoint[\"model\"].items()\n            }\n        self.model.load_state_dict(\n            state_dict=checkpoint[\"model\"],\n            strict=True,\n        )\n        self.model.eval()\n        # self.print(\"Finish loading checkpoint.\")\n\n    def init_dumper(\n        self, need_atom_confidence: bool = False, sorted_by_ranking_score: bool = True\n    ):\n        self.dumper = DataDumper(\n            base_dir=self.dump_dir,\n            need_atom_confidence=need_atom_confidence,\n            sorted_by_ranking_score=sorted_by_ranking_score,\n        )\n\n    def print_dict(self, d):\n        for k, v in d.items():\n            if isinstance(v, torch.Tensor):\n                print(f\"{k}: \", v.shape)\n            else:\n                pass\n                # print(f\"{k}: {v}\")\n\n    # Adapted from runner.train.Trainer.evaluate\n    @torch.no_grad()\n    def predict(self, data: Mapping[str, Mapping[str, Any]]) -> dict[str, torch.Tensor]:\n        eval_precision = {\n            \"fp32\": torch.float32,\n            \"bf16\": torch.bfloat16,\n            \"fp16\": torch.float16,\n        }[self.configs.dtype]\n        # print(\"eval_precision: \", eval_precision)\n        enable_amp = (\n            torch.autocast(device_type=\"cuda\", dtype=eval_precision)\n            if torch.cuda.is_available()\n            else nullcontext()\n        )\n        #         print('input_feature_dict: ', self.print_dict(data[\"input_feature_dict\"]))\n        #         exit(0)\n\n        data = to_device(data, self.device)\n        with enable_amp:\n            prediction, _, _ = self.model(\n                input_feature_dict=data[\"input_feature_dict\"],\n                label_full_dict=None,\n                label_dict=None,\n                mode=\"inference\",\n            )\n\n        return prediction\n\n    def print(self, msg: str):\n        if DIST_WRAPPER.rank == 0:\n            # logger.info(msg)\n            print(msg)\n\n    def update_model_configs(self, new_configs: Any) -> None:\n        self.model.configs = new_configs\n\n\ndef update_inference_configs(configs: Any, N_token: int):\n    # Setting the default inference configs for different N_token and N_atom\n    # when N_token is larger than 3000, the default config might OOM even on a\n    # A100 80G GPUS,\n    if N_token > 3840:\n        configs.skip_amp.confidence_head = False\n        configs.skip_amp.sample_diffusion = False\n    elif N_token > 2560:\n        configs.skip_amp.confidence_head = False\n        configs.skip_amp.sample_diffusion = True\n    else:\n        configs.skip_amp.confidence_head = True\n        configs.skip_amp.sample_diffusion = True\n    return configs\n\n\ndef infer_predict(runner: InferenceRunner, configs: Any) -> None:\n    # Data\n    # logger.info(f\"Loading data from {configs.input_json_path}\")\n    try:\n        dataloader = get_inference_dataloader(configs=configs)\n    except Exception as e:\n        error_message = f\"{e}:\\n{traceback.format_exc()}\"\n        logger.info(error_message)\n        with open(opjoin(runner.error_dir, \"error.txt\"), \"a\") as f:\n            f.write(error_message)\n        return\n\n    num_data = len(dataloader.dataset)\n    for seed in configs.seeds:\n        seed_everything(seed=seed, deterministic=configs.deterministic)\n        for batch in dataloader:\n            try:\n                data, atom_array, data_error_message = batch[0]\n                sample_name = data[\"sample_name\"]\n\n                if len(data_error_message) > 0:\n                    logger.info(data_error_message)\n                    with open(opjoin(runner.error_dir, f\"{sample_name}.txt\"), \"a\") as f:\n                        f.write(data_error_message)\n                    continue\n\n                logger.info(\n                    (\n                        f\"[Rank {DIST_WRAPPER.rank} ({data['sample_index'] + 1}/{num_data})] {sample_name}: \"\n                        f\"N_asym {data['N_asym'].item()}, N_token {data['N_token'].item()}, \"\n                        f\"N_atom {data['N_atom'].item()}, N_msa {data['N_msa'].item()}\"\n                    )\n                )\n                new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n                runner.update_model_configs(new_configs)\n                prediction = runner.predict(data)\n                runner.dumper.dump(\n                    dataset_name=\"\",\n                    pdb_id=sample_name,\n                    seed=seed,\n                    pred_dict=prediction,\n                    atom_array=atom_array,\n                    entity_poly_type=data[\"entity_poly_type\"],\n                )\n\n                logger.info(\n                    f\"[Rank {DIST_WRAPPER.rank}] {data['sample_name']} succeeded - \"\n                    f\"Results saved to {configs.dump_dir}\"\n                )\n                torch.cuda.empty_cache()\n            except Exception as e:\n                error_message = f\"[Rank {DIST_WRAPPER.rank}]{data['sample_name']} {e}:\\n{traceback.format_exc()}\"\n                logger.info(error_message)\n                # Save error info\n                with open(opjoin(runner.error_dir, f\"{sample_name}.txt\"), \"a\") as f:\n                    f.write(error_message)\n                if hasattr(torch.cuda, \"empty_cache\"):\n                    torch.cuda.empty_cache()\n\n\n# data helper\ndef make_dummy_solution(valid_df):\n    solution = dotdict()\n    for i, row in valid_df.iterrows():\n        target_id = row.target_id\n        sequence = row.sequence\n        solution[target_id] = dotdict(\n            target_id=target_id,\n            sequence=sequence,\n            coord=[],\n        )\n    return solution\n\n\ndef solution_to_submit_df(solution):\n    submit_df = []\n    for k, s in solution.items():\n        df = coord_to_df(s.sequence, s.coord, s.target_id)\n        submit_df.append(df)\n\n    submit_df = pd.concat(submit_df)\n    return submit_df\n\n\ndef coord_to_df(sequence, coord, target_id):\n    L = len(sequence)\n    df = pd.DataFrame()\n    df[\"ID\"] = [f\"{target_id}_{i + 1}\" for i in range(L)]\n    df[\"resname\"] = [s for s in sequence]\n    df[\"resid\"] = [i + 1 for i in range(L)]\n\n    num_coord = len(coord)\n    for j in range(num_coord):\n        df[f\"x_{j+1}\"] = coord[j][:, 0]\n        df[f\"y_{j+1}\"] = coord[j][:, 1]\n        df[f\"z_{j+1}\"] = coord[j][:, 2]\n    return df\n\n\ndef main(configs: Any) -> None:\n    # Runner\n    runner = InferenceRunner(configs)\n    infer_predict(runner, configs)\n\n\ndef create_input_json(sequence, target_id):\n    # print(\"input_no_msa\")\n    input_json = [\n        {\n            \"sequences\": [\n                {\n                    \"rnaSequence\": {\n                        \"sequence\": sequence,\n                        \"count\": 1,\n                    }\n                }\n            ],\n            \"name\": target_id,\n        }\n    ]\n    return input_json\n\n\ndef extract_c1_coordinates(cif_file_path):\n    try:\n        # Read the CIF file using the correct biotite method\n        with open(cif_file_path, \"r\") as f:\n            cif_data = pdbx.CIFFile.read(f)\n\n        # Get structure from CIF data\n        atom_array = pdbx.get_structure(cif_data, model=1)\n\n        # Clean atom names and find C1' atoms\n        atom_names_clean = np.char.strip(atom_array.atom_name.astype(str))\n        mask_c1 = atom_names_clean == \"C1'\"\n        c1_atoms = atom_array[mask_c1]\n\n        if len(c1_atoms) == 0:\n            print(f\"Warning: No C1' atoms found in {cif_file_path}\")\n            return None\n\n        # Sort by residue ID and return coordinates\n        sort_indices = np.argsort(c1_atoms.res_id)\n        c1_atoms_sorted = c1_atoms[sort_indices]\n        c1_coords = c1_atoms_sorted.coord\n\n        return c1_coords\n    except Exception as e:\n        print(f\"Error extracting C1' coordinates from {cif_file_path}: {e}\")\n        return None\n\n\ndef process_sequence(sequence, target_id, temp_dir):\n    # Create input JSON\n    input_json = create_input_json(sequence, target_id)\n\n    # Save JSON to temporary file\n    os.makedirs(temp_dir, exist_ok=True)\n    input_json_path = os.path.join(temp_dir, f\"{target_id}_input.json\")\n    with open(input_json_path, \"w\") as f:\n        json.dump(input_json, f, indent=4)\n\n\ndef run_ptx(target_id, sequence, configs, solution, template_idx, runner):\n    # Create directories\n    temp_dir = f\"./{configs.dump_dir}/input\"  # Same as in kaggle_inference.py\n    output_dir = f\"./{configs.dump_dir}/output\"  # Same as in kaggle_inference.py\n    os.makedirs(temp_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n\n    process_sequence(sequence=sequence, target_id=target_id, temp_dir=temp_dir)\n    configs.input_json_path = os.path.join(temp_dir, f\"{target_id}_input.json\")\n    configs.template_idx = int(template_idx)\n\n    infer_predict(runner, configs)\n\n    cif_file_path = (\n        f\"{configs.dump_dir}/{target_id}/seed_42/predictions/{target_id}_sample_0.cif\"\n    )\n    cif_new_path = f\"{configs.dump_dir}/{target_id}/seed_42/predictions/{target_id}_sample_{template_idx}_new.cif\"\n    shutil.copy(cif_file_path, cif_new_path)\n    coord = extract_c1_coordinates(cif_file_path)\n    if coord is None:\n        coord = np.zeros((len(sequence), 3), dtype=np.float32)\n    elif coord.shape[0] < (len(sequence)):\n        pad_len = len(sequence) - coord.shape[0]\n        pad = np.zeros((pad_len, 3), dtype=np.float32)\n        coord = np.concatenate([coord, pad], axis=0)\n    solution[target_id].coord.append(coord)\n\n\ndef run() -> None:\n    LOG_FORMAT = \"%(asctime)s,%(msecs)-3d %(levelname)-8s [%(filename)s:%(lineno)s %(funcName)s] %(message)s\"\n    logging.basicConfig(\n        format=LOG_FORMAT,\n        level=logging.WARNING,\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        filemode=\"w\",\n    )\n    # Silence dataloader and rnapro module logging\n    logging.getLogger(\"rnapro.data\").setLevel(logging.WARNING)\n    logging.getLogger(\"rnapro\").setLevel(logging.WARNING)\n    configs_base[\"use_deepspeed_evo_attention\"] = (\n        os.environ.get(\"USE_DEEPSPEED_EVO_ATTENTION\", False) == \"true\"\n    )\n    configs = {**configs_base, **{\"data\": data_configs}, **inference_configs}\n    configs = parse_configs(\n        configs=configs,\n        arg_str=parse_sys_args(),\n        fill_required_with_null=True,\n    )\n\n    valid_df = pd.read_csv(configs.sequences_csv)\n    print(f\"\\n -> Loaded {len(valid_df)} sequence(s)\")\n\n    # Build model and load checkpoint once before looping over sequences\n\n    print('\\n -> Building model and loading checkpoint')\n    runner = InferenceRunner(configs)\n    print('\\n -> Done, starting inference...')\n\n    solution = make_dummy_solution(valid_df)\n    for idx, row in valid_df.iterrows():\n        print(f\"\\n -> Sequence {row.target_id}: {row.sequence}\")\n\n        if len(row.sequence) > configs.max_len:\n            print(f'Sequence is too long ({len(row.sequence)} > {configs.max_len}), skipping')\n            for template_idx in range(5):\n                coord = np.zeros((len(row.sequence), 3), dtype=np.float32)\n                solution[row.target_id].coord.append(coord)\n            continue\n\n        try:\n            target_id = row.target_id\n            sequence = row.sequence\n            for template_idx in range(5):\n                print()\n                run_ptx(\n                    target_id=target_id,\n                    sequence=sequence,\n                    configs=configs,\n                    solution=solution,\n                    template_idx=template_idx,\n                    runner=runner,\n                )\n        except Exception as e:\n            print(f\"Error processing {row.target_id}: {e}\")\n            continue\n\n    print('\\n\\n -> Inference done ! Saving to submission.csv')\n    submit_df = solution_to_submit_df(solution)\n    submit_df = submit_df.fillna(0.0)\n    submit_df.to_csv(\"./submission.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    run()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2026-01-15T22:52:36.139453Z","iopub.status.busy":"2026-01-15T22:52:36.139086Z","iopub.status.idle":"2026-01-15T22:52:36.150983Z","shell.execute_reply":"2026-01-15T22:52:36.150299Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.240087,"end_time":"2026-01-15T22:52:36.152415","exception":false,"start_time":"2026-01-15T22:52:35.912328","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference script\n- Where the parameters are specified","metadata":{"papermill":{"duration":0.223898,"end_time":"2026-01-15T22:52:36.592426","exception":false,"start_time":"2026-01-15T22:52:36.368528","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile rnapro_inference_kaggle.sh\n\nexport LAYERNORM_TYPE=torch # fast_layernorm, torch\n\n\n# Inference parameters (RNAPro)\nSEED=42\nN_SAMPLE=1\nN_STEP=200\nN_CYCLE=10\n\n# Paths\nDUMP_DIR=\"../output\"\n# Set a valid checkpoint file path below\nCHECKPOINT_PATH=\"../rnapro-private-best-500m.ckpt\"\n\n# Template/MSA settings\nTEMPLATE_DATA=\"./release_data/kaggle/templates.pt\"\n# Note: template_idx supports 5 choices and maps to top-k:\n# 0->top1, 1->top2, 2->top3, 3->top4, 4->top5\nTEMPLATE_IDX=0\nRNA_MSA_DIR=\"/kaggle/input/stanford-rna-3d-folding-2/MSA\"\n\n# SEQUENCES_CSV=\"/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv\"\nSEQUENCES_CSV=\"/kaggle/working/sample_sequences.csv\"\n\n# RibonanzaNet2 path (keep as-is per request)\nRIBONANZA_PATH=\"/kaggle/input/ribonanzanet2/pytorch/alpha/1/\"\n\n# Model selection: keep to an existing key to align defaults (N_step=200, N_cycle=10)\nMODEL_NAME=\"rnapro_base\"\nmkdir -p \"${DUMP_DIR}\"\n\npython3 runner/inference.py \\\n    --model_name \"${MODEL_NAME}\" \\\n    --seeds ${SEED} \\\n    --dump_dir \"${DUMP_DIR}\" \\\n    --load_checkpoint_path \"${CHECKPOINT_PATH}\" \\\n    --use_msa true \\\n    --use_template \"ca_precomputed\" \\\n    --model.use_template \"ca_precomputed\" \\\n    --model.use_RibonanzaNet2 true \\\n    --model.template_embedder.n_blocks 2 \\\n    --model.ribonanza_net_path \"${RIBONANZA_PATH}\" \\\n    --template_data \"${TEMPLATE_DATA}\" \\\n    --template_idx ${TEMPLATE_IDX} \\\n    --rna_msa_dir \"${RNA_MSA_DIR}\" \\\n    --model.N_cycle ${N_CYCLE} \\\n    --sample_diffusion.N_sample ${N_SAMPLE} \\\n    --sample_diffusion.N_step ${N_STEP} \\\n    --load_strict true \\\n    --num_workers 0 \\\n    --triangle_attention \"torch\" \\\n    --triangle_multiplicative \"torch\" \\\n    --sequences_csv \"${SEQUENCES_CSV}\" \\\n    --max_len 1000\n\n\n# --triangle_attention supports 'triattention', 'cuequivariance', 'deepspeed', 'torch'\n# --triangle_multiplicative supports 'cuequivariance', 'torch'\n# --max_len 1000: Sequences longer than max_len will be skipped to avoid oom","metadata":{"execution":{"iopub.execute_input":"2026-01-15T22:52:37.025925Z","iopub.status.busy":"2026-01-15T22:52:37.025235Z","iopub.status.idle":"2026-01-15T22:52:37.03069Z","shell.execute_reply":"2026-01-15T22:52:37.029939Z"},"papermill":{"duration":0.225419,"end_time":"2026-01-15T22:52:37.032055","exception":false,"start_time":"2026-01-15T22:52:36.806636","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run !","metadata":{"papermill":{"duration":0.304625,"end_time":"2026-01-15T22:52:37.573143","exception":false,"start_time":"2026-01-15T22:52:37.268518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!bash ./rnapro_inference_kaggle.sh","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2026-01-15T22:52:38.012879Z","iopub.status.busy":"2026-01-15T22:52:38.012332Z","iopub.status.idle":"2026-01-15T23:04:45.438839Z","shell.execute_reply":"2026-01-15T23:04:45.438119Z"},"papermill":{"duration":727.648981,"end_time":"2026-01-15T23:04:45.440608","exception":false,"start_time":"2026-01-15T22:52:37.791627","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv submission.csv ..","metadata":{"execution":{"iopub.execute_input":"2026-01-15T23:04:45.874838Z","iopub.status.busy":"2026-01-15T23:04:45.874513Z","iopub.status.idle":"2026-01-15T23:04:45.994887Z","shell.execute_reply":"2026-01-15T23:04:45.99413Z"},"papermill":{"duration":0.339126,"end_time":"2026-01-15T23:04:45.996398","exception":false,"start_time":"2026-01-15T23:04:45.657272","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.execute_input":"2026-01-15T23:04:46.431843Z","iopub.status.busy":"2026-01-15T23:04:46.431139Z","iopub.status.idle":"2026-01-15T23:04:46.436243Z","shell.execute_reply":"2026-01-15T23:04:46.435479Z"},"papermill":{"duration":0.223984,"end_time":"2026-01-15T23:04:46.43761","exception":false,"start_time":"2026-01-15T23:04:46.213626","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!head submission.csv","metadata":{"execution":{"iopub.execute_input":"2026-01-15T23:04:46.95393Z","iopub.status.busy":"2026-01-15T23:04:46.953327Z","iopub.status.idle":"2026-01-15T23:04:47.07039Z","shell.execute_reply":"2026-01-15T23:04:47.069699Z"},"papermill":{"duration":0.415157,"end_time":"2026-01-15T23:04:47.071737","exception":false,"start_time":"2026-01-15T23:04:46.65658","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Done !","metadata":{"papermill":{"duration":0.215086,"end_time":"2026-01-15T23:04:47.501983","exception":false,"start_time":"2026-01-15T23:04:47.286897","status":"completed"},"tags":[]}}]}