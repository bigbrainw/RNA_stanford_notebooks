{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae83f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import random\n",
    "from Bio.Align import PairwiseAligner\n",
    "from Bio.Seq import Seq\n",
    "import time\n",
    "from scipy.spatial import distance_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a global aligner instance with the same parameters as pairwise2\n",
    "aligner = PairwiseAligner()\n",
    "aligner.mode = 'global'\n",
    "aligner.match_score = 2.0\n",
    "aligner.mismatch_score = -1.0\n",
    "aligner.open_gap_score = -8.0\n",
    "aligner.extend_gap_score = -0.3\n",
    "\n",
    "def get_aligned_sequences(alignment):\n",
    "    \"\"\"Extract aligned sequences with gaps from Bio.Align alignment object.\"\"\"\n",
    "    # Get the original sequences\n",
    "    query_seq = str(alignment.query)\n",
    "    target_seq = str(alignment.target)\n",
    "    \n",
    "    # Get aligned segments (indices where sequences align)\n",
    "    query_aligned = alignment.aligned[0]  # query sequence aligned segments\n",
    "    target_aligned = alignment.aligned[1]  # target sequence aligned segments\n",
    "    \n",
    "    # Build gapped sequences\n",
    "    a_q = []\n",
    "    a_t = []\n",
    "    \n",
    "    q_idx = 0\n",
    "    t_idx = 0\n",
    "    \n",
    "    for seg_q, seg_t in zip(query_aligned, target_aligned):\n",
    "        # Add gaps in query before this aligned segment\n",
    "        while q_idx < seg_q[0]:\n",
    "            a_q.append(query_seq[q_idx])\n",
    "            a_t.append('-')\n",
    "            q_idx += 1\n",
    "        \n",
    "        # Add gaps in target before this aligned segment\n",
    "        while t_idx < seg_t[0]:\n",
    "            a_q.append('-')\n",
    "            a_t.append(target_seq[t_idx])\n",
    "            t_idx += 1\n",
    "        \n",
    "        # Add aligned segment\n",
    "        seg_len = seg_q[1] - seg_q[0]\n",
    "        for i in range(seg_len):\n",
    "            a_q.append(query_seq[seg_q[0] + i])\n",
    "            a_t.append(target_seq[seg_t[0] + i])\n",
    "        \n",
    "        q_idx = seg_q[1]\n",
    "        t_idx = seg_t[1]\n",
    "    \n",
    "    # Add remaining characters\n",
    "    while q_idx < len(query_seq):\n",
    "        a_q.append(query_seq[q_idx])\n",
    "        a_t.append('-')\n",
    "        q_idx += 1\n",
    "    \n",
    "    while t_idx < len(target_seq):\n",
    "        a_q.append('-')\n",
    "        a_t.append(target_seq[t_idx])\n",
    "        t_idx += 1\n",
    "    \n",
    "    return ''.join(a_q), ''.join(a_t)\n",
    "\n",
    "# === 1. LOADING ===\n",
    "DATA_PATH = '/kaggle/input/stanford-rna-3d-folding-2/'\n",
    "\n",
    "# Load base data\n",
    "train_seqs = pd.read_csv(DATA_PATH + 'train_sequences.csv')\n",
    "test_seqs = pd.read_csv(DATA_PATH + 'test_sequences.csv')\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train_labels.csv')\n",
    "\n",
    "# ===== ONLY CHANGE: TRY TO LOAD VALIDATION =====\n",
    "try:\n",
    "    validation_seqs = pd.read_csv(DATA_PATH + 'validation_sequences.csv')\n",
    "    validation_labels = pd.read_csv(DATA_PATH + 'validation_labels.csv')\n",
    "    print(\"Validation data found and will be combined with train data.\")\n",
    "    \n",
    "    # Combine sequences\n",
    "    combined_seqs = pd.concat([train_seqs, validation_seqs], ignore_index=True)\n",
    "    \n",
    "    # Combine labels\n",
    "    combined_labels = pd.concat([train_labels, validation_labels], ignore_index=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Validation data not found, using only train data.\")\n",
    "    combined_seqs = train_seqs\n",
    "    combined_labels = train_labels\n",
    "# ===== END OF CHANGE =====\n",
    "\n",
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    for id_prefix, group in labels_df.groupby(lambda x: labels_df['ID'][x].rsplit('_', 1)[0]):\n",
    "        coords = [group.sort_values('resid')[['x_1', 'y_1', 'z_1']].values]\n",
    "        coords_dict[id_prefix] = coords[0]\n",
    "    return coords_dict\n",
    "\n",
    "# Use COMBINED data instead of only train\n",
    "combined_coords_dict = process_labels(combined_labels)\n",
    "\n",
    "# === 2. HEURISTICS (unchanged) ===\n",
    "\n",
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    similar_seqs = []\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    \n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id, train_seq = row['target_id'], row['sequence']\n",
    "        if target_id not in train_coords_dict: \n",
    "            continue\n",
    "        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.4: \n",
    "            continue\n",
    "        \n",
    "        # FIX 1: Slightly changed gap penalties (-10 -> -8, -0.5 -> -0.3)\n",
    "        # This helps better match templates that have gaps in loop regions\n",
    "        # Using Bio.Align.PairwiseAligner instead of deprecated pairwise2\n",
    "        alignments = list(aligner.align(query_seq_obj, train_seq))\n",
    "        \n",
    "        if alignments:\n",
    "            score = alignments[0].score / (2 * min(len(query_seq), len(train_seq)))\n",
    "            similar_seqs.append((target_id, train_seq, score, train_coords_dict[target_id]))\n",
    "    \n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similar_seqs[:top_n]\n",
    "\n",
    "def adaptive_rna_constraints(coordinates, sequence, confidence=1.0):\n",
    "    refined_coords = coordinates.copy()\n",
    "    n_residues = len(sequence)\n",
    "    \n",
    "    # FIX 2: Optimized the tension strength.\n",
    "    # For good templates (conf > 0.8) it becomes almost zero.\n",
    "    constraint_strength = 0.7 * (1.0 - min(confidence, 0.95))\n",
    "    \n",
    "    # FIX 3: Narrowed the target distance range (was 5.5-6.5, now 5.8-6.1)\n",
    "    # This makes the chain more \"springy\"/consistent\n",
    "    seq_min_dist, seq_max_dist = 5.8, 6.1\n",
    "    \n",
    "    for i in range(n_residues - 1):\n",
    "        dist = np.linalg.norm(refined_coords[i+1] - refined_coords[i])\n",
    "        if dist < seq_min_dist or dist > seq_max_dist:\n",
    "            target_dist = 5.95 \n",
    "            direction = (refined_coords[i+1] - refined_coords[i]) / (dist + 1e-10)\n",
    "            adjustment = (target_dist - dist) * constraint_strength\n",
    "            refined_coords[i+1] = refined_coords[i+1] + direction * adjustment\n",
    "            \n",
    "    return refined_coords\n",
    "\n",
    "def adapt_template_to_query(query_seq, template_seq, template_coords):\n",
    "    # Adaptation code from the original version (most stable one)\n",
    "    # Using Bio.Align.PairwiseAligner instead of deprecated pairwise2\n",
    "    alignments = list(aligner.align(Seq(query_seq), Seq(template_seq)))\n",
    "    if not alignments: \n",
    "        return np.zeros((len(query_seq), 3))\n",
    "    \n",
    "    # Extract aligned sequences with gaps from the new API\n",
    "    alignment = alignments[0]\n",
    "    a_q, a_t = get_aligned_sequences(alignment)\n",
    "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
    "    q_idx, t_idx = 0, 0\n",
    "    for char_q, char_t in zip(a_q, a_t):\n",
    "        if char_q != '-' and char_t != '-':\n",
    "            if t_idx < len(template_coords): \n",
    "                new_coords[q_idx] = template_coords[t_idx]\n",
    "            q_idx += 1\n",
    "            t_idx += 1\n",
    "        elif char_q != '-': \n",
    "            q_idx += 1\n",
    "        elif char_t != '-': \n",
    "            t_idx += 1\n",
    "\n",
    "    # Fill NaN values\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            prev_v = next((j for j in range(i-1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            next_v = next((j for j in range(i+1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            if prev_v >= 0 and next_v >= 0:\n",
    "                w = (i - prev_v) / (next_v - prev_v)\n",
    "                new_coords[i] = (1 - w) * new_coords[prev_v] + w * new_coords[next_v]\n",
    "            elif prev_v >= 0: \n",
    "                new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n",
    "            elif next_v >= 0: \n",
    "                new_coords[i] = new_coords[next_v] + [3, 0, 0]\n",
    "            else: \n",
    "                new_coords[i] = [i * 3, 0, 0]\n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "def generate_rna_structure(sequence, seed=None):\n",
    "    if seed: \n",
    "        np.random.seed(seed)\n",
    "    n = len(sequence)\n",
    "    coords = np.zeros((n, 3))\n",
    "    for i in range(1, n):\n",
    "        coords[i] = coords[i-1] + [random.uniform(3.8, 4.2), 0, 0]\n",
    "    return coords\n",
    "\n",
    "# === 3. PREDICT (unchanged, but uses combined data) ===\n",
    "\n",
    "def predict_rna_structures(sequence, target_id, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    predictions = []\n",
    "    similar_seqs = find_similar_sequences(sequence, train_seqs_df, train_coords_dict, top_n=n_predictions)\n",
    "    \n",
    "    if similar_seqs:\n",
    "        for i, (template_id, template_seq, similarity, template_coords) in enumerate(similar_seqs):\n",
    "            adapted = adapt_template_to_query(sequence, template_seq, template_coords)\n",
    "            refined = adaptive_rna_constraints(adapted, sequence, confidence=similarity)\n",
    "            \n",
    "            # FIX 4: Reduced noise level for top templates (0.05 -> 0.02)\n",
    "            # This keeps the best of the 5 predictions more accurate\n",
    "            random_scale = max(0.01, (0.4 - similarity) * 0.1) \n",
    "            refined += np.random.normal(0, random_scale, refined.shape)\n",
    "            predictions.append(refined)\n",
    "                \n",
    "    while len(predictions) < n_predictions:\n",
    "        predictions.append(generate_rna_structure(sequence, seed=len(predictions)))\n",
    "    \n",
    "    return predictions[:n_predictions]\n",
    "\n",
    "# === 4. LOOP & SAVE (unchanged) ===\n",
    "all_predictions = []\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    target_id, sequence = row['target_id'], row['sequence']\n",
    "    if idx % 5 == 0: \n",
    "        print(f\"Processing {idx+1}/{len(test_seqs)}\")\n",
    "    \n",
    "    # HERE we use COMBINED data instead of train_seqs and train_coords_dict\n",
    "    preds = predict_rna_structures(sequence, target_id, combined_seqs, combined_coords_dict)\n",
    "    \n",
    "    for j in range(len(sequence)):\n",
    "        res = {'ID': f\"{target_id}_{j+1}\", 'resname': sequence[j], 'resid': j+1}\n",
    "        for i in range(5):\n",
    "            res[f'x_{i+1}'], res[f'y_{i+1}'], res[f'z_{i+1}'] = preds[i][j]\n",
    "        all_predictions.append(res)\n",
    "\n",
    "submission_df = pd.DataFrame(all_predictions)\n",
    "cols = ['ID', 'resname', 'resid'] + [f'{c}_{i}' for i in range(1,6) for c in ['x','y','z']]\n",
    "submission_df[cols].to_csv('submission.csv', index=False)\n",
    "print(\"Submission.csv generated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4eed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import random\n",
    "from Bio.Align import PairwiseAligner\n",
    "from Bio.Seq import Seq\n",
    "import time\n",
    "from scipy.spatial import distance_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a global aligner instance with the same parameters as pairwise2\n",
    "aligner = PairwiseAligner()\n",
    "aligner.mode = 'global'\n",
    "aligner.match_score = 2.0\n",
    "aligner.mismatch_score = -1.0\n",
    "aligner.open_gap_score = -8.0\n",
    "aligner.extend_gap_score = -0.3\n",
    "\n",
    "def get_aligned_sequences(alignment):\n",
    "    \"\"\"Extract aligned sequences with gaps from Bio.Align alignment object.\"\"\"\n",
    "    # Get the original sequences\n",
    "    query_seq = str(alignment.query)\n",
    "    target_seq = str(alignment.target)\n",
    "    \n",
    "    # Get aligned segments (indices where sequences align)\n",
    "    query_aligned = alignment.aligned[0]  # query sequence aligned segments\n",
    "    target_aligned = alignment.aligned[1]  # target sequence aligned segments\n",
    "    \n",
    "    # Build gapped sequences\n",
    "    a_q = []\n",
    "    a_t = []\n",
    "    \n",
    "    q_idx = 0\n",
    "    t_idx = 0\n",
    "    \n",
    "    for seg_q, seg_t in zip(query_aligned, target_aligned):\n",
    "        # Add gaps in query before this aligned segment\n",
    "        while q_idx < seg_q[0]:\n",
    "            a_q.append(query_seq[q_idx])\n",
    "            a_t.append('-')\n",
    "            q_idx += 1\n",
    "        \n",
    "        # Add gaps in target before this aligned segment\n",
    "        while t_idx < seg_t[0]:\n",
    "            a_q.append('-')\n",
    "            a_t.append(target_seq[t_idx])\n",
    "            t_idx += 1\n",
    "        \n",
    "        # Add aligned segment\n",
    "        seg_len = seg_q[1] - seg_q[0]\n",
    "        for i in range(seg_len):\n",
    "            a_q.append(query_seq[seg_q[0] + i])\n",
    "            a_t.append(target_seq[seg_t[0] + i])\n",
    "        \n",
    "        q_idx = seg_q[1]\n",
    "        t_idx = seg_t[1]\n",
    "    \n",
    "    # Add remaining characters\n",
    "    while q_idx < len(query_seq):\n",
    "        a_q.append(query_seq[q_idx])\n",
    "        a_t.append('-')\n",
    "        q_idx += 1\n",
    "    \n",
    "    while t_idx < len(target_seq):\n",
    "        a_q.append('-')\n",
    "        a_t.append(target_seq[t_idx])\n",
    "        t_idx += 1\n",
    "    \n",
    "    return ''.join(a_q), ''.join(a_t)\n",
    "\n",
    "# === 1. LOADING ===\n",
    "DATA_PATH = '/kaggle/input/stanford-rna-3d-folding-2/'\n",
    "\n",
    "# Load base data\n",
    "train_seqs = pd.read_csv(DATA_PATH + 'train_sequences.csv')\n",
    "test_seqs = pd.read_csv(DATA_PATH + 'test_sequences.csv')\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train_labels.csv')\n",
    "\n",
    "# ===== ONLY CHANGE: TRY TO LOAD VALIDATION =====\n",
    "try:\n",
    "    validation_seqs = pd.read_csv(DATA_PATH + 'validation_sequences.csv')\n",
    "    validation_labels = pd.read_csv(DATA_PATH + 'validation_labels.csv')\n",
    "    print(\"Validation data found and will be combined with train data.\")\n",
    "    \n",
    "    # Combine sequences\n",
    "    combined_seqs = pd.concat([train_seqs, validation_seqs], ignore_index=True)\n",
    "    \n",
    "    # Combine labels\n",
    "    combined_labels = pd.concat([train_labels, validation_labels], ignore_index=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Validation data not found, using only train data.\")\n",
    "    combined_seqs = train_seqs\n",
    "    combined_labels = train_labels\n",
    "# ===== END OF CHANGE =====\n",
    "\n",
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    for id_prefix, group in labels_df.groupby(lambda x: labels_df['ID'][x].rsplit('_', 1)[0]):\n",
    "        coords = [group.sort_values('resid')[['x_1', 'y_1', 'z_1']].values]\n",
    "        coords_dict[id_prefix] = coords[0]\n",
    "    return coords_dict\n",
    "\n",
    "# Use COMBINED data instead of only train\n",
    "combined_coords_dict = process_labels(combined_labels)\n",
    "\n",
    "# === 2. HEURISTICS (unchanged) ===\n",
    "\n",
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    similar_seqs = []\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    \n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id, train_seq = row['target_id'], row['sequence']\n",
    "        if target_id not in train_coords_dict: \n",
    "            continue\n",
    "        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.4: \n",
    "            continue\n",
    "        \n",
    "        # FIX 1: Slightly changed gap penalties (-10 -> -8, -0.5 -> -0.3)\n",
    "        # This helps better match templates that have gaps in loop regions\n",
    "        # Using Bio.Align.PairwiseAligner instead of deprecated pairwise2\n",
    "        alignments = list(aligner.align(query_seq_obj, train_seq))\n",
    "        \n",
    "        if alignments:\n",
    "            score = alignments[0].score / (2 * min(len(query_seq), len(train_seq)))\n",
    "            similar_seqs.append((target_id, train_seq, score, train_coords_dict[target_id]))\n",
    "    \n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similar_seqs[:top_n]\n",
    "\n",
    "def adaptive_rna_constraints(coordinates, sequence, confidence=1.0):\n",
    "    refined_coords = coordinates.copy()\n",
    "    n_residues = len(sequence)\n",
    "    \n",
    "    # FIX 2: Optimized the tension strength.\n",
    "    # For good templates (conf > 0.8) it becomes almost zero.\n",
    "    constraint_strength = 0.7 * (1.0 - min(confidence, 0.95))\n",
    "    \n",
    "    # FIX 3: Narrowed the target distance range (was 5.5-6.5, now 5.8-6.1)\n",
    "    # This makes the chain more \"springy\"/consistent\n",
    "    seq_min_dist, seq_max_dist = 5.8, 6.1\n",
    "    \n",
    "    for i in range(n_residues - 1):\n",
    "        dist = np.linalg.norm(refined_coords[i+1] - refined_coords[i])\n",
    "        if dist < seq_min_dist or dist > seq_max_dist:\n",
    "            target_dist = 5.95 \n",
    "            direction = (refined_coords[i+1] - refined_coords[i]) / (dist + 1e-10)\n",
    "            adjustment = (target_dist - dist) * constraint_strength\n",
    "            refined_coords[i+1] = refined_coords[i+1] + direction * adjustment\n",
    "            \n",
    "    return refined_coords\n",
    "\n",
    "def adapt_template_to_query(query_seq, template_seq, template_coords):\n",
    "    # Adaptation code from the original version (most stable one)\n",
    "    # Using Bio.Align.PairwiseAligner instead of deprecated pairwise2\n",
    "    alignments = list(aligner.align(Seq(query_seq), Seq(template_seq)))\n",
    "    if not alignments: \n",
    "        return np.zeros((len(query_seq), 3))\n",
    "    \n",
    "    # Extract aligned sequences with gaps from the new API\n",
    "    alignment = alignments[0]\n",
    "    a_q, a_t = get_aligned_sequences(alignment)\n",
    "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
    "    q_idx, t_idx = 0, 0\n",
    "    for char_q, char_t in zip(a_q, a_t):\n",
    "        if char_q != '-' and char_t != '-':\n",
    "            if t_idx < len(template_coords): \n",
    "                new_coords[q_idx] = template_coords[t_idx]\n",
    "            q_idx += 1\n",
    "            t_idx += 1\n",
    "        elif char_q != '-': \n",
    "            q_idx += 1\n",
    "        elif char_t != '-': \n",
    "            t_idx += 1\n",
    "\n",
    "    # Fill NaN values\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            prev_v = next((j for j in range(i-1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            next_v = next((j for j in range(i+1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            if prev_v >= 0 and next_v >= 0:\n",
    "                w = (i - prev_v) / (next_v - prev_v)\n",
    "                new_coords[i] = (1 - w) * new_coords[prev_v] + w * new_coords[next_v]\n",
    "            elif prev_v >= 0: \n",
    "                new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n",
    "            elif next_v >= 0: \n",
    "                new_coords[i] = new_coords[next_v] + [3, 0, 0]\n",
    "            else: \n",
    "                new_coords[i] = [i * 3, 0, 0]\n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "def generate_rna_structure(sequence, seed=None):\n",
    "    if seed: \n",
    "        np.random.seed(seed)\n",
    "    n = len(sequence)\n",
    "    coords = np.zeros((n, 3))\n",
    "    for i in range(1, n):\n",
    "        coords[i] = coords[i-1] + [random.uniform(3.8, 4.2), 0, 0]\n",
    "    return coords\n",
    "\n",
    "# === 3. PREDICT (unchanged, but uses combined data) ===\n",
    "\n",
    "def predict_rna_structures(sequence, target_id, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    predictions = []\n",
    "    similar_seqs = find_similar_sequences(sequence, train_seqs_df, train_coords_dict, top_n=n_predictions)\n",
    "    \n",
    "    if similar_seqs:\n",
    "        for i, (template_id, template_seq, similarity, template_coords) in enumerate(similar_seqs):\n",
    "            adapted = adapt_template_to_query(sequence, template_seq, template_coords)\n",
    "            refined = adaptive_rna_constraints(adapted, sequence, confidence=similarity)\n",
    "            \n",
    "            # FIX 4: Reduced noise level for top templates (0.05 -> 0.02)\n",
    "            # This keeps the best of the 5 predictions more accurate\n",
    "            random_scale = max(0.01, (0.4 - similarity) * 0.1) \n",
    "            refined += np.random.normal(0, random_scale, refined.shape)\n",
    "            predictions.append(refined)\n",
    "                \n",
    "    while len(predictions) < n_predictions:\n",
    "        predictions.append(generate_rna_structure(sequence, seed=len(predictions)))\n",
    "    \n",
    "    return predictions[:n_predictions]\n",
    "\n",
    "# === 4. LOOP & SAVE (unchanged) ===\n",
    "all_predictions = []\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    target_id, sequence = row['target_id'], row['sequence']\n",
    "    if idx % 5 == 0: \n",
    "        print(f\"Processing {idx+1}/{len(test_seqs)}\")\n",
    "    \n",
    "    # HERE we use COMBINED data instead of train_seqs and train_coords_dict\n",
    "    preds = predict_rna_structures(sequence, target_id, combined_seqs, combined_coords_dict)\n",
    "    \n",
    "    for j in range(len(sequence)):\n",
    "        res = {'ID': f\"{target_id}_{j+1}\", 'resname': sequence[j], 'resid': j+1}\n",
    "        for i in range(5):\n",
    "            res[f'x_{i+1}'], res[f'y_{i+1}'], res[f'z_{i+1}'] = preds[i][j]\n",
    "        all_predictions.append(res)\n",
    "\n",
    "submission_df = pd.DataFrame(all_predictions)\n",
    "cols = ['ID', 'resname', 'resid'] + [f'{c}_{i}' for i in range(1,6) for c in ['x','y','z']]\n",
    "submission_df[cols].to_csv('submission.csv', index=False)\n",
    "print(\"Submission.csv generated!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
